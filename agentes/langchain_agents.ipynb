{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from openai import OpenAI\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#client = OpenAI(api_key=getenv('API_KEY_DALLE'))\n",
    "\n",
    "#Nombre del despliegue en OpenAI Studio, el modelo es 'text-davinci-003'\n",
    "#engine = 'Davinci003'\n",
    "model = 'gpt-3.5-turbo-instruct'\n",
    "openai_api_version = '2023-12-01'\n",
    "\n",
    "#Nombre del despliegue para el modelo de embeddings es 'TextEmbeddingAda002'\n",
    "embeddings_engine = 'TextEmbeddingAda002'\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = OpenAI(openai_api_key=getenv('OPEN_API_KEY', ''),\n",
    "                 model= model,\n",
    "                 #engine=engine,\n",
    "                 #openai_api_version=openai_api_version,\n",
    "                 temperature=0.2)\n",
    "\n",
    "llm.max_tokens = max_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - ¿Qué es un agente?\n",
    "\n",
    "El concepto de \"agente\" es una abstracción a nuestro modelo de lenguaje:\n",
    "\n",
    "    * Utilizar calculadoras\n",
    "    * Utilizar calendarios\n",
    "    * Ejecutar código Python\n",
    "    * Buscar en Internet\n",
    "    * etc.\n",
    "\n",
    "Este tipo deacciones se realizarán mediante herramientas:\n",
    "\n",
    "    * Proporcionadas por LangChain\n",
    "    * Diseñadas por nosotros mismos (como veremos más adelante)\n",
    "\n",
    "Por lo tanto, un agente está compuesto de dos elementos principales:\n",
    "\n",
    "    * Un LLM base\n",
    "    * Una o múltiples herramientas con las que interactuar\n",
    "\n",
    "Dependiendo de la entrada del usuario, el agente puede decidir cuál, si alguna, de estas herramientas llamar. La utilizdad de LangChain se hace evidente en este contexto, ya que proporciona un marco de programación versátil para construir tales sistemas.\n",
    "\n",
    "## 1.2 - ¿Qué es una herramienta?\n",
    "En LangChain, las herramientas son funciones que pueden ser llamadas por un agente. Se utilizan para realizar tareas específicas, como acceder a datos, generar texto o traducir idiomas.\n",
    "\n",
    "Las herramientas se definen como objetos `Tool` que ocntienen tres propiedades:\n",
    "\n",
    "    * name: El nombre de la herramienta\n",
    "    * description: Una descripción de la herramienta\n",
    "    * function: La función a ejecutar cuando se llama a la herramienta\n",
    "\n",
    "Las herramientas se utilizan para facilitar la creación de agentes complejos que pueden realizar una amplia gama de tareas. P. ej., un agente que ayuda a los usuarios a resolver problemas técnicos podría usar herramientas para acceder a la documentación, buscar soluciones en línea o generar código\n",
    "\n",
    "A continuación, se presentan algunas herramientas comunes:\n",
    "    * Wikipedia\n",
    "    * DuckDuckGoSearch\n",
    "    * ShellTool\n",
    "\n",
    "Sin embargo, hay una gran cantidad de ellas en la doc oficial de LangChain (y su nº aumenta cada día).\n",
    "\n",
    "\n",
    "### Wikipedia\n",
    "\n",
    "Para búsquedas medianamente estructuradas en Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: OpenAI\\nSummary: OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\\nAs one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\nPage: Sora (text-to-video model)\\nSummary: Sora is an upcoming generative artificial intelligence model developed by  OpenAI, that specializes in text-to-video generation. The model accepts textual descriptions, known as prompts, from users and generates short video clips corresponding to those descriptions. Prompts can specify artistic styles, fantastical imagery, or real-world scenarios.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from wikipedia import set_lang as wiki_lang\n",
    "\n",
    "wiki_lang('en')\n",
    "\n",
    "wikipedia_eng = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(\n",
    "    doc_content_chars_max = 2000\n",
    "))\n",
    "\n",
    "wikipedia_eng.run('Open AI')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: OpenAI\\nSummary: OpenAI es una empresa de investigación y despliegue de inteligencia artificial que declara tener como misión asegurar que la inteligencia artificial general beneficie a toda la humanidad.[4]\\u200b\\nAlgunas de sus áreas de enfoque son los modelos generativos, la seguridad, la robótica y el lenguaje natural. OpenAI también ofrece una plataforma de API que permite a los usuarios acceder y construir sus propios modelos de IA personalizados.\\nOpenAI fue fundada en 2015 por Ilya Sutskever, Greg Brockman, Trevor Blackwell, Andrej Karpathy y con Sam Altman y Elon Musk como miembros iniciales de la junta.[5]\\u200b[6]\\u200b Se fundó principalmente como una organización sin fines de lucro, pero en 2019 se convirtió en una empresa híbrida, con una entidad sin fines de lucro y otra con fines de lucro.\\nAlgunos de sus proyectos más destacados son ChatGPT, un chatbot (Modelo de lenguaje) que puede ver, oír y hablar[7]\\u200b; DALL·E 3, un modelo de imágenes que puede crear imágenes a partir de descripciones de texto[8]\\u200b; y GPT-4, su modelo de lenguaje más avanzado y capaz.[9]\\u200b\\nMicrosoft proporcionó a OpenAI una inversión de mil millones de dólares en 2019 y una inversión de diez mil millones de dólares en 2023.[10]\\u200b[11]\\u200b Los sistemas de OpenAI se ejecutan en una plataforma de supercomputación basada en Azure de Microsoft.[12]\\u200b[13]\\u200b[14]\\u200b\\n\\nPage: OpenAI Five\\nSummary: OpenAI Five es un programa informático de OpenAI que juega al videojuego de cinco contra cinco Dota 2. Su primera aparición pública tuvo lugar en 2017, donde fue probado en una partida en vivo de uno contra uno contra el jugador profesional Dendi, que perdió contra él. Al año siguiente, el sistema había avanzado hasta el punto de actuar como un equipo completo de cinco personas, y empezó a jugar contra equipos profesionales y a demostrar su capacidad para derrotarlos.\\nAl elegir un juego tan complejo como Dota 2 para estudiar el aprendizaje automático, OpenAI pensó que podría capturar con mayor precisión la imprevisibilidad'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_lang('es')\n",
    "\n",
    "wiki_es = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(\n",
    "    lang='es',\n",
    "    doc_content_chars_max=2000\n",
    "))\n",
    "\n",
    "wiki_es.run('Open AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DuckDuckGoSearch\n",
    "\n",
    "Para búsquedas \"raw\" en Internet\n",
    "\n",
    "`Nota:` En este caso, no he encontrado la manera de indicarle que haga las búsquedas en español, pero puede haber alguna forma. Sino, siempre podemos traducir el resultado como paso intermedio a que nuestro modelo lo procese.\n",
    "\n",
    "pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Careers at OpenAI. Developing safe and beneficial AI requires people from a wide range of disciplines and backgrounds. I encourage my team to keep learning. Ideas in different topics or fields can often inspire new ideas and broaden the potential solution space. Lilian Weng Safety Systems at OpenAI. OpenAI unveiled Voice Engine, an A.I. technology that can recreate a person's voice from a 15-second recording. Amazon said it had added $2.75 billion to its investment in Anthropic, ... Feb. 16, 2024. OpenAI has completed a deal that values the San Francisco artificial intelligence company at $80 billion or more, nearly tripling its valuation in less than 10 months, according to ... OpenAI is an artificial intelligence research company that produces services such as ChatGPT. Here's what you need to know. Artificial intelligence, or AI, is actively changing how we interact with technology. Generative AI is a type of AI technology that can produce text, images, audio, and more, when prompted by a human-usually using ... Oct. 20, 2023. OpenAI is in talks to complete a deal that would value the company at $80 billion or more, nearly triple its valuation less than six months ago, according to a person with knowledge ...\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\curl_cffi\\aio.py:205: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.run(\"Open AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shell\n",
    "\n",
    "Permite a los agentes ejecutar comandos de shell. Esto puede ser muy útil para tareas que requieren acceso al SO, como interactuar con el sistema de archivos o ejecutar apps.\n",
    "\n",
    "`Nota:`\n",
    "* No funciona en Windows\n",
    "* Es peligroso dejar el control de nuestro sistema de archivos, lo mejor sería encapsularlo en una máq virtual y prohibirle ciertos comandos (por si acaso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " [\"echo 'Hello World'\", 'dir']\n",
      "'Hello World';dir\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:32: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "print(shell_tool.run(\n",
    "    {\n",
    "        \"commands\": [\n",
    "            \"echo 'Hello World'\",\n",
    "            \"dir\"\n",
    "        ]\n",
    "    }\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Tipos de agentes\n",
    "\n",
    "Hay dos tipos de agentes principales:\n",
    "* Agentes de Acción: deciden una acción a tomar y realizan esa acción paso a paso\n",
    "* Agentes de Planificación y Ejecución (Experimental): estos agentes primero deciden un plan de acciones a tomar y luego ejecutan esas acciones paso a paso\n",
    "\n",
    "\n",
    "# 2 - Agentes de Acción\n",
    "\n",
    "Pseudocódigo de alto nivel de los agentes de acción:\n",
    "1. Se recibe una entrada del usuario\n",
    "2. El agente decide qué herramienta utilizar, si es que hay alguna, y cuál debe ser la entrada de esa herramienta\n",
    "3. A continuación, se llama a esa herramienta con esa entrada de herramienta y se registra una observación (esto es simplemente la salida de llamar a esa herramienta con esa entrada de herramienta).\n",
    "4. Ese historial, la entrada de herramienta y observación final se devuelve al agente, y éste decide qué paso dar a continuación.\n",
    "5. Esto se repite hasta que el agente decide que ya no necesita utilizar ninguna herramienta, y entonces responde directamente al usuario.\n",
    "\n",
    "`4 tipos de agentes de acción (inspirados del paper: \"ReAct: Synergizing Reasoning and Acting in Language Models\")`:\n",
    "\n",
    "* ReAct Zero-shot\n",
    "* ReAct Conversational\n",
    "* ReAct Docstore\n",
    "* ReAct Self-ask\n",
    "\n",
    "AgentTypes en la doc de LangChain.\n",
    "\n",
    "\n",
    "## 2.1 - ReAct Zero-shot\n",
    "\n",
    "pip install numexpr\n",
    "\n",
    "`Nota importante: ` Al interactuar con agentes, es muy importante establecer el parámetro `max_iterations` porque los agentes pueden quedarse atrapados en bucles infinitos que consumen muchos tokens. El valor predeterminado es 15 para permitir el uso de muchas herramientas y razonamientos complejos, pero paara la mayoría de las apps es mejor mantenerlo mucho más bajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentType, Tool, load_tools, initialize_agent\n",
    "\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"llm-math\"],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "\n",
    "zero_shot_agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this exponent problem\n",
      "Action: Calculator\n",
      "Action Input: 101^9\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 1093685272684360901\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1093685272684360901\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¿Cuánto es 101 elevado a 9', 'output': '1093685272684360901'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent_executor(\"¿Cuánto es 101 elevado a 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos, el agente utiliza el inglés como idioma \"base\". Como hemos visto en otras clases, podemos modificar este comportamiento modificando los prompts de la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(zero_shot_agent_executor.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, vamos a tomar un enfoque diferente y vamos a traducir el texto de español a inglés antes de pasárselo al agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def call_agent_with_translate(agent, es_eng_chain, eng_es_chain, query):\n",
    "    print(f'Input: {query}')\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        query_translated = es_eng_chain(query)['text']\n",
    "        query_translated = query_translated.replace('\\n', '')\n",
    "        es_eng_token_count = cb.total_tokens\n",
    "    print(f'Input (traducido al inglés): {query_translated}')\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        result = agent(query_translated)\n",
    "        agent_token_count = cb.total_tokens\n",
    "    print(f\"Output: {result['output']}\")\n",
    "\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        output_translated = eng_es_chain(result['output'])['text']\n",
    "        output_translated = output_translated.replace('\\n', '')\n",
    "        eng_es_token_count = cb.total_tokens\n",
    "    print(f'Output (traducido al español): {output_translated}')\n",
    "\n",
    "    total_tokens = es_eng_token_count + agent_token_count + eng_es_token_count\n",
    "\n",
    "    print(f'He usado un total de {total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "#Chain para traducir el texto de entrada de español a inglés\n",
    "translate_eng_es_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Translate the following spanish text to english (write only the translated text): {input}\"\n",
    "    )\n",
    "\n",
    "translate_eng_es_chain = LLMChain(llm=llm, prompt=translate_eng_es_prompt)\n",
    "\n",
    "    #Chain para traducir el texto de salida del modelo de inglés a español\n",
    "translate_es_eng_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Si el siguiente texto está en inglés, tradúcelo al español, en caso contrario, simplemente devuelve el ismo texto: {input}\"\n",
    "    )\n",
    "\n",
    "translate_es_eng_chain = LLMChain(llm=llm, prompt=translate_es_eng_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Podrías decirme cuánto es 8 x 21?\n",
      "Input (traducido al inglés): Could you tell me how much is 8 x 21?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to multiply 8 by 21.\n",
      "Action: Calculator\n",
      "Action Input: 8 x 21\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 168\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 168\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: 168\n",
      "Output (traducido al español): 168\n",
      "He usado un total de 658 tokens\n"
     ]
    }
   ],
   "source": [
    "result = call_agent_with_translate(\n",
    "    zero_shot_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Podrías decirme cuánto es 8 x 21?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - ReAct Conversacional\n",
    "\n",
    "El agente zero-shot es interesante, pero no tiene memoria. ¿Qué pasa si queremos un asistente que recuerde las cosas que hemos hablado y que también pueda razonar sobre ellas y utilizar herramientas? Para eso tenemos el agente ReAct Conversacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Podrías decirme cuál es el resultado de 100 + 25?\n",
      "Input (traducido al inglés): Could you tell me what is the result of 100 + 25?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 100 + 25\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 125\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The result of 100 + 25 is 125.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: The result of 100 + 25 is 125.\n",
      "Output (traducido al español): El resultado de 100 + 25 es 125.\n",
      "He usado un total de 1166 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"llm-math\"],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\")\n",
    "\n",
    "conversational_agent_executor = initialize_agent(\n",
    "    agent = AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = call_agent_with_translate(\n",
    "    conversational_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Podrías decirme cuál es el resultado de 100 + 25?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Qué es lo último que te he preguntado en nuestra conversación?\n",
      "Input (traducido al inglés): What was the last thing I asked you in our conversation?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAI: The last thing you asked me was \"Could you tell me what is the result of 100 + 25?\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: The last thing you asked me was \"Could you tell me what is the result of 100 + 25?\"\n",
      "Output (traducido al español): Lo último que me preguntaste fue \"¿Podrías decirme cuál es el resultado de 100 + 25?\"\n",
      "He usado un total de 558 tokens\n"
     ]
    }
   ],
   "source": [
    "result = call_agent_with_translate(\n",
    "    conversational_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Qué es lo último que te he preguntado en nuestra conversación?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - ReAct Docstore\n",
    "\n",
    "Este tipo de agente es similar a los vistos hasta ahora, pero incluye la interacción con un almacén de docs. Solo tendrá dos herramientas a su disposición:\n",
    "* `Search` (Búsqueda). El agente usa esta herramienta para encontrar un artículo relevante\n",
    "* `Lookup` (Consulta). El agente intenta encontrar la info correcta del artículo\n",
    "Es el agente por defecto en el caso que queramos hacer búsquedas simples en wikipedia o almacenes de docs similares.\n",
    "\n",
    "`Nota`: ReAct Docstore no está pensado para trabajar con memoria por defecto. En tal caso, tendríamos que preparar nuestro propio Agente customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain.agents.react.base.DocstoreExplorer` was deprecated in langchain 0.1.0 and will be removed in 0.2.0\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from wikipedia import set_lang as wiki_lang\n",
    "\n",
    "wiki_lang('en')\n",
    "\n",
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func = docstore.search,\n",
    "        description=\"search wikipedia\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func = docstore.lookup,\n",
    "        description='lookup a term in wikipedia'\n",
    "    )\n",
    "]\n",
    "\n",
    "docstore_agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.REACT_DOCSTORE,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El prompt asociado al modelo difiere de los anteriores en cuanto que contiene varios ejs del bucle Pregunta > Pensamiento > Acción > Observación, que incluyen las herramientas de búsqueda y consulta.\n",
    "\n",
    "`Nota`: Este bucle se explica con más detalle (al igual que con más ejs en el paper original `ReAct: Synergizing Reasoning and Acting in Language Models`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
      "Thought: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
      "Action: Search[Colorado orogeny]\n",
      "Observation: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
      "Thought: It does not mention the eastern sector. So I need to look up eastern sector.\n",
      "Action: Lookup[eastern sector]\n",
      "Observation: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
      "Thought: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
      "Action: Search[High Plains]\n",
      "Observation: High Plains refers to one of two distinct land regions\n",
      "Thought: I need to instead search High Plains (United States).\n",
      "Action: Search[High Plains (United States)]\n",
      "Observation: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
      "Thought: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
      "Action: Finish[1,800 to 7,000 ft]\n",
      "\n",
      "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
      "Thought: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
      "Action: Search[Milhouse]\n",
      "Observation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
      "Thought: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
      "Action: Lookup[named after]\n",
      "Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
      "Thought: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
      "Action: Finish[Richard Nixon]\n",
      "\n",
      "Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n",
      "Thought: I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\n",
      "Action: Search[Adam Clayton Powell]\n",
      "Observation: Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell (film)’, ’Giancarlo Esposito’].\n",
      "Thought: To find the documentary, I can search Adam Clayton Powell (film).\n",
      "Action: Search[Adam Clayton Powell (film)]\n",
      "Observation: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\n",
      "Thought: Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\n",
      "Action: Finish[The Saimaa Gesture]\n",
      "\n",
      "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
      "Thought: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
      "Action: Search[Nicholas Ray]\n",
      "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
      "Thought: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
      "Action: Search[Elia Kazan]\n",
      "Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
      "Thought: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
      "Action: Finish[director, screenwriter, actor]\n",
      "\n",
      "Question: Which magazine was started first Arthur’s Magazine or First for Women?\n",
      "Thought: I need to search Arthur’s Magazine and First for Women, and find which was started first.\n",
      "Action: Search[Arthur’s Magazine]\n",
      "Observation: Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
      "Thought: Arthur’s Magazine was started in 1844. I need to search First for Women next.\n",
      "Action: Search[First for Women]\n",
      "Observation: First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
      "Thought: First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for Women), so Arthur’s Magazine was started first.\n",
      "Action: Finish[Arthur’s Magazine]\n",
      "\n",
      "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
      "Thought: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
      "Action: Search[Pavel Urysohn]\n",
      "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
      "Thought: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
      "Action: Search[Leonid Levin]\n",
      "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
      "Thought: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
      "Action: Finish[yes]\n",
      "\n",
      "\n",
      "Question: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(docstore_agent_executor.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Cuáles han sido las principales investigaciones del filósofo griego Aristóteles?\n",
      "Input (traducido al inglés): What have been the main investigations of the Greek philosopher Aristotle?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search Aristotle and find his main investigations.\n",
      "Action: Search[Aristotle]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAristotle (384–322 BC) was an Ancient Greek philosopher and polymath. His writings cover a broad range of subjects spanning the natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts. As the founder of the Peripatetic school of philosophy in the Lyceum in Athens, he began the wider Aristotelian tradition that followed, which set the groundwork for the development of modern science.\n",
      "Little is known about Aristotle's life. He was born in the city of Stagira in northern Greece during the Classical period. His father, Nicomachus, died when Aristotle was a child, and he was brought up by a guardian. At 17 or 18, he joined Plato's Academy in Athens and remained there until the age of 37 (c. 347 BC). Shortly after Plato died, Aristotle left Athens and, at the request of Philip II of Macedon, tutored his son Alexander the Great beginning in 343 BC. He established a library in the Lyceum, which helped him to produce many of his hundreds of books on papyrus scrolls.\n",
      "Though Aristotle wrote many elegant treatises and dialogues for publication, only around a third of his original output has survived, none of it intended for publication. Aristotle provided a complex synthesis of the various philosophies existing prior to him. His teachings and methods of inquiry have had a significant impact across the world, and remain a subject of contemporary philosophical discussion.\n",
      "Aristotle's views profoundly shaped medieval scholarship. The influence of his physical science extended from late antiquity and the Early Middle Ages into the Renaissance, and was not replaced systematically until the Enlightenment and theories such as classical mechanics were developed. He influenced Judeo-Islamic philosophies during the Middle Ages, as well as Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Catholic Church.\n",
      "Aristotle was revered among medieval Muslim scholars as \"The First Teacher\", and among medieval Christians like Thomas Aquinas as simply \"The Philosopher\", while the poet Dante called him \"the master of those who know\". His works contain the earliest known formal study of logic, and were studied by medieval scholars such as Peter Abelard and Jean Buridan. Aristotle's influence on logic continued well into the 19th century. In addition, his ethics, although always influential, gained renewed interest with the modern advent of virtue ethics.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Aristotle's main investigations were in natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts. \n",
      "Action: Finish[natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts\n",
      "Output (traducido al español): ciencias naturales, filosofía, lingüística, economía, política, psicología y artes\n",
      "He usado un total de 3457 tokens\n"
     ]
    }
   ],
   "source": [
    "result = call_agent_with_translate(\n",
    "    docstore_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Cuáles han sido las principales investigaciones del filósofo griego Aristóteles?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - ReAct Self-ask\n",
    "\n",
    "Es nuestra primera elección de agente para cuando queremos utilizar un LLM con el objetivo de extraer info con un motor de búsqueda. El agente hará preguntas de seguimiento y utilizará la funcionalidad de búsqueda para obtener respuestas intermedias que lo ayuden a llegar a una respuesta final.\n",
    "\n",
    "Utilizaremos el motor de búsqueda DuckDuckGo porque es gratuito y no requiere una clave.\n",
    "\n",
    "`Nota`: Al igual que ReAct Docstore, ReAct Self-ask no está pensado para trabajar con memoria por defecto. En tal caso, tendríamos que preparar nuestro propio Agente customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Intermediate Answer\",\n",
    "        func = search.run,\n",
    "        description=\"duckduckgo search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search_agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.SELF_ASK_WITH_SEARCH,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver a continuación, el prompt del agente es básicamente una serie de ejs para mostrarle al LLM cómo hacer preguntas de seguimiento a una herramienta de búsqueda hasta que pueda llegar a la respuesta final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(self_ask_with_search_agent_executor.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Sabes el nombre del futbolista con más balones de oro?\n",
      "Input (traducido al inglés): Do you know the name of the football player with the most golden balls?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: What are golden balls in football?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mThe Ballon d'Or (French pronunciation: [balɔ̃ dɔʁ] ⓘ; lit. 'Golden Ball ') is an annual football award presented by French news magazine France Football since 1956 to honour the player deemed to have performed the best over the previous season. Conceived by sports writers Gabriel Hanot and Jacques Ferran, the Ballon d'Or award was based exclusively on voting by football journalists, from ... The Ballon d'Or, or \"Golden Ball\" in French, is the most prestigious individual award in world football, recognising the world's finest male and female players. The Golden Ball has been awarded to some of the game's best players, including Diego Maradona, Lionel Messi, Cristiano Ronaldo, Zinedine Zidane, and many others. Football is the world's most beloved sport, with millions of fans across the globe. The Ballon d'Or, also known as the Golden Ball, is the premier individual award in football and is presented annually to the best-performing male footballer. The Ballon d'Or, which translates from French to \"Golden Ball\", is the most prestigious individual award in world football, honoring the best men's and women's player across the globe for that season. Meaning of Ballon D'Or. The term \"Ballon D'Or\" translates to \"Golden Ball\" in English. This symbol of excellence represents the pinnacle of achievement in soccer. Winning the Ballon D'Or is a testament to a player's extraordinary talent and dedication to the sport.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: Who is the football player with the most Ballon d'Or awards?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mThe player with the most Ballon d'Or awards of all-time is Lionel Messi who has won it eight times. Behind him is Messi's long-time rival Cristiano Ronaldo with five Ballons d'Or. Only 10 players ... The brainchild of France Football magazine journalist Gabriel Hanot, the Ballon d'Or is widely seen as the most prestigious award in the footballing calendar.. Between 1956 and 1995 the ... Who has won the most women's Ballon d'Or titles? The Ballon d'Or Féminin has only been awarded since 2018 and in 2020 the awards were not held, so the list of winners is a short one. Ballon d'Or winners receive a different level of respect from those in the football world. For players like Lionel Messi and Cristiano Ronaldo, Ballon d'Or awards have helped solidify their careers as two of the greatest players that have ever graced the pitch. Related: Meet USMNT Midfielder Malik Tillman. There have been 25 players who have ... The Ballon d'Or (French pronunciation: [balɔ̃ dɔʁ] ⓘ; lit. 'Golden Ball ') is an annual football award presented by French news magazine France Football since 1956 to honour the player deemed to have performed the best over the previous season. Conceived by sports writers Gabriel Hanot and Jacques Ferran, the Ballon d'Or award was based exclusively on voting by football journalists, from ...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: Who is the football player with the most Ballon d'Or awards?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mThe player with the most Ballon d'Or awards of all-time is Lionel Messi who has won it eight times. Behind him is Messi's long-time rival Cristiano Ronaldo with five Ballons d'Or. Only 10 players ... The brainchild of France Football magazine journalist Gabriel Hanot, the Ballon d'Or is widely seen as the most prestigious award in the footballing calendar.. Between 1956 and 1995 the ... Who has won the most women's Ballon d'Or titles? The Ballon d'Or Féminin has only been awarded since 2018 and in 2020 the awards were not held, so the list of winners is a short one. The Ballon d'Or (French pronunciation: [balɔ̃ dɔʁ] ⓘ; lit. 'Golden Ball ') is an annual football award presented by French news magazine France Football since 1956 to honour the player deemed to have performed the best over the previous season. Conceived by sports writers Gabriel Hanot and Jacques Ferran, the Ballon d'Or award was based exclusively on voting by football journalists, from ... Michael Platini leads the podium of the most prolific winners of the award, as the French player won it three times. While playing for Juventus, the former UEFA president won the Ballon d'Or three ...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: Lionel Messi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: Lionel Messi\n",
      "Output (traducido al español): Lionel Messi\n",
      "He usado un total de 3610 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\curl_cffi\\aio.py:205: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    }
   ],
   "source": [
    "question = 'Sabes el nombre del futbolista con más balones de oro?'\n",
    "\n",
    "result = call_agent_with_translate(\n",
    "    self_ask_with_search_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Sabes en qué equpos ha jugado Messi?\n",
      "Input (traducido al inglés): Do you know which teams Messi has played for?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  No, the question is clear and does not require any additional information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:1166\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:731\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m--> 731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\output_parsers\\self_ask.py:41\u001b[0m, in \u001b[0;36mSelfAskOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish_string \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m last_line:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AgentFinish({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: last_line[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish_string) :]}, text)\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse output:  No, the question is clear and does not require any additional information.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSabes en qué equpos ha jugado Messi?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcall_agent_with_translate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_ask_with_search_agent_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtranslate_eng_es_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtranslate_es_eng_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mcall_agent_with_translate\u001b[1;34m(agent, es_eng_chain, eng_es_chain, query)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput (traducido al inglés): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_translated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[1;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_translated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     agent_token_count \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mtotal_tokens\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:1432\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1432\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1441\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1442\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1148\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:1138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1148\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain\\agents\\agent.py:1177\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1182\u001b[0m     )\n\u001b[0;32m   1183\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  No, the question is clear and does not require any additional information."
     ]
    }
   ],
   "source": [
    "question = 'Sabes en qué equpos ha jugado Messi?'\n",
    "\n",
    "result = call_agent_with_translate(\n",
    "    self_ask_with_search_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    question\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Agentes de Planificación y Ejecución\n",
    "\n",
    "Los agentes `Plan-and-Execute` (PaE) logran un objetivo planificando primero qué hacer y luiego ejecutando las subtareas. Esta idea se inspira en gran medida en `BabyAGI` y luego en el doc `Plan and Solve`\n",
    "\n",
    "* La planificación casi siempre la realiza un LLM\n",
    "* La ejecución generalmente la realiza un agente de acción (equipado con herramientas)\n",
    "\n",
    "Pseudocódigo de alto nivel de los agentes PaE:\n",
    "* Se recibe alguna entrada del usuario\n",
    "* El `planificador` enumera los pasos a seguir\n",
    "* El `ejecutor` recorre la lista de pasos, ejecutándolos\n",
    "\n",
    "Forma típica de construir un agente PaE:\n",
    "* `Planificador`: actua a modo de \"cerebro\" y define qué pasos se deben seguir. Se puede invocar mediante la función `load_chat_planner(llm)`\n",
    "    - LLM\n",
    "* `Ejecutor`: decide qué herramientas llamar en cada paso y en qué orden. Se puede invocar con la función `load_agent_executor(llm, tools)`\n",
    "    - LLM\n",
    "    - Kit de herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Search',\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    #Esta es la manera \"formal\" de llamar a 'llm-math'\n",
    "    Tool(\n",
    "        name='Calculator',\n",
    "        func=llm_math_chain.run,\n",
    "        description='useful for when you need to answer questions about math'\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "\n",
    "planner = load_chat_planner(llm)\n",
    "executor = load_agent_executor(llm, tools, verbose=True)\n",
    "pae_agent_executor = PlanAndExecute(planner=planner, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Calcula el valor de pi elevado a 6. Dime el resultado del cálculo.\n",
      "Input (traducido al inglés): Calculate the value of pi raised to the 6th power. Tell me the result of the calculation.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The value of pi is approximately 3.141592653589793.\"\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"3.141592653589793 ** 6\"\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"round(244.140625, 2)\"\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Rishi Sunak is 43 years old.\"\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: Action:\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Rishi Sunak is 43 years old.\"\n",
      "}\n",
      "\n",
      "\n",
      "Output (traducido al español): Acción:{  \"acción\": \"Respuesta final\",  \"acción_entrada\": \"Rishi Sunak tiene 43 años.\"}\n",
      "He usado un total de 11796 tokens\n"
     ]
    }
   ],
   "source": [
    "question = \"Calcula el valor de pi elevado a 6. Dime el resultado del cálculo.\"\n",
    "\n",
    "result = call_agent_with_translate(\n",
    "    pae_agent_executor,\n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    question\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain-NIv1GH6l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
