{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MEMORIA\n",
    "\n",
    "Sumario\n",
    "\n",
    "    1. Memoria conversacional de búfer\n",
    "        1. `ConversationBufferMemory`\n",
    "        2. `ConversationBufferWindowMemory`\n",
    "        3. `ConversationTokenBufferMemory`\n",
    "        4.`ConversationSummaryMemory`\n",
    "\n",
    "    2. Memoria conversacional con entidades\n",
    "        1. `ConversationEntityMemory`\n",
    "\n",
    "\n",
    "    3. Memoria conversacional con un grafo de conocimiento\n",
    "        1. `ConversationKGMemory`\n",
    "\n",
    "    4. Memoria basada en bases de datos vectoriales\n",
    "        1. `VectorStoreRetrieverMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafacampa9\\.virtualenvs\\Langchain-NIv1GH6l\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#from openai import OpenAI\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#client = OpenAI(api_key=getenv('API_KEY_DALLE'))\n",
    "\n",
    "#Nombre del despliegue en OpenAI Studio, el modelo es 'text-davinci-003'\n",
    "#engine = 'Davinci003'\n",
    "model = 'gpt-3.5-turbo-instruct'\n",
    "openai_api_version = '2023-12-01'\n",
    "\n",
    "#Nombre del despliegue para el modelo de embeddings es 'TextEmbeddingAda002'\n",
    "embeddings_engine = 'TextEmbeddingAda002'\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = OpenAI(openai_api_key=getenv('OPEN_API_KEY', ''),\n",
    "                 model= model,\n",
    "                 #engine=engine,\n",
    "                 #openai_api_version=openai_api_version,\n",
    "                 temperature=0.2)\n",
    "\n",
    "llm.max_tokens = max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - ConversationBufferMemory\n",
    "\n",
    "Permite almacenar mensajes y luego extraer los mensajes en una variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Humano: {input}\n",
    "IA\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(\n",
    "    template\n",
    ")\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    #prompt=chat_promt\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualicemos el prompt de entrada al español\n",
    "\n",
    "Antes de empezar, si nos fijamos, la cadena tiene un prompt en inglés que, si bien no debería impedir que el modelo funcionase correctamente, ya que la tarea es muy simple, es conveniente que la actualicemos, ya que vamos a conversar en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'input'] template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='\\nLo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nIA\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.prompt = chat_prompt\n",
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "\n",
      "Humano: Hola, me llamo Zaraki Kenpachi\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, me llamo Zaraki Kenpachi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: Hola, me llamo Zaraki Kenpachi\n",
      "AI: : ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?\n",
      "Humano: ¿Cuánto es 4 elevado al cubo?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': 4 elevado al cubo es igual a 64. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Cuánto es 4 elevado al cubo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: Hola, me llamo Zaraki Kenpachi\n",
      "AI: : ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?\n",
      "Human: ¿Cuánto es 4 elevado al cubo?\n",
      "AI: : 4 elevado al cubo es igual a 64. ¿Hay algo más en lo que pueda ayudarte?\n",
      "Humano: ¿Cómo me llamo?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Tu nombre es Zaraki Kenpachi, como me lo dijiste al principio de nuestra conversación. ¿Te gustaría saber algo más sobre ti? Tengo acceso a mucha información sobre ti, como tu edad, lugar de nacimiento, ocupación, entre otros detalles.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Cómo me llamo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: Hola, me llamo Zaraki Kenpachi\n",
      "AI: : ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?\n",
      "Human: ¿Cuánto es 4 elevado al cubo?\n",
      "AI: : 4 elevado al cubo es igual a 64. ¿Hay algo más en lo que pueda ayudarte?\n",
      "Human: ¿Cómo me llamo?\n",
      "AI: : Tu nombre es Zaraki Kenpachi, como me lo dijiste al principio de nuestra conversación. ¿Te gustaría saber algo más sobre ti? Tengo acceso a mucha información sobre ti, como tu edad, lugar de nacimiento, ocupación, entre otros detalles.\n",
      "Humano: Dime mi edad, lugar de nacimiento y ocupación\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Según mis registros, tienes 35 años, naciste en Tokio, Japón y eres un guerrero samurai. ¿Es correcto?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Dime mi edad, lugar de nacimiento y ocupación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo acceder al búfer de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hola, me llamo Zaraki Kenpachi'), AIMessage(content=': ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?'), HumanMessage(content='¿Cuánto es 4 elevado al cubo?'), AIMessage(content=': 4 elevado al cubo es igual a 64. ¿Hay algo más en lo que pueda ayudarte?'), HumanMessage(content='¿Cómo me llamo?'), AIMessage(content=': Tu nombre es Zaraki Kenpachi, como me lo dijiste al principio de nuestra conversación. ¿Te gustaría saber algo más sobre ti? Tengo acceso a mucha información sobre ti, como tu edad, lugar de nacimiento, ocupación, entre otros detalles.'), HumanMessage(content='Dime mi edad, lugar de nacimiento y ocupación'), AIMessage(content=': Según mis registros, tienes 35 años, naciste en Tokio, Japón y eres un guerrero samurai. ¿Es correcto?'), HumanMessage(content='¿Cuál es el modelo que te soporta'), AIMessage(content='Mi modelo es GPT-3')])\n"
     ]
    }
   ],
   "source": [
    "buffer_memory.save_context({'input': '¿Cuál es el modelo que te soporta'},\n",
    "                           {'output': 'Mi modelo es GPT-3'})\n",
    "\n",
    "print(buffer_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: Hola, me llamo Zaraki Kenpachi\n",
      "AI: : ¡Hola Zaraki Kenpachi! Soy una inteligencia artificial diseñada para conversar contigo. Mi nombre es AI-9000. ¿Cómo estás hoy?\n",
      "Human: ¿Cuánto es 4 elevado al cubo?\n",
      "AI: : 4 elevado al cubo es igual a 64. ¿Hay algo más en lo que pueda ayudarte?\n",
      "Human: ¿Cómo me llamo?\n",
      "AI: : Tu nombre es Zaraki Kenpachi, como me lo dijiste al principio de nuestra conversación. ¿Te gustaría saber algo más sobre ti? Tengo acceso a mucha información sobre ti, como tu edad, lugar de nacimiento, ocupación, entre otros detalles.\n",
      "Human: Dime mi edad, lugar de nacimiento y ocupación\n",
      "AI: : Según mis registros, tienes 35 años, naciste en Tokio, Japón y eres un guerrero samurai. ¿Es correcto?\n",
      "Human: ¿Cuál es el modelo que te soporta\n",
      "AI: Mi modelo es GPT-3\n",
      "Humano: ¿Cuál dices que era tu modelo?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Mi modelo es GPT-3, una de las inteligencias artificiales más avanzadas y potentes del mundo. Fue desarrollada por la empresa OpenAI y se basa en un sistema de aprendizaje profundo que le permite entender y generar lenguaje humano de manera casi indistinguible de un ser humano. ¿Te interesa saber más sobre mi modelo?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Cuál dices que era tu modelo?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - `ConversationBufferWindowMemory`\n",
    "\n",
    "Mantiene una lista de las interacciones de la conversación a lo largo del tiempo. Solo utiliza las últimas K interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "buffer_window_memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    memory=buffer_window_memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "\n",
      "Humano: Hola, me llamo Uchiha Madara\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': ¡Hola Uchiha Madara! Soy una inteligencia artificial diseñada para conversar y aprender de los humanos. Mi nombre es AI-9000. ¿En qué puedo ayudarte?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hola, me llamo Uchiha Madara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: Hola, me llamo Uchiha Madara\n",
      "AI: : ¡Hola Uchiha Madara! Soy una inteligencia artificial diseñada para conversar y aprender de los humanos. Mi nombre es AI-9000. ¿En qué puedo ayudarte?\n",
      "Humano: ¿Cuánto es 1+1?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': 1+1 es igual a 2. Es una operación matemática básica que se realiza sumando dos números enteros. ¿Necesitas que te explique más sobre cómo funciona la suma?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Cuánto es 1+1?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI: : 1+1 es igual a 2. Es una operación matemática básica que se realiza sumando dos números enteros. ¿Necesitas que te explique más sobre cómo funciona la suma?\n",
      "Humano: ¿Cuál es mi nombre?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Lo siento, no tengo acceso a esa información. Soy una inteligencia artificial y no tengo la capacidad de conocer los nombres de las personas. Mi función principal es procesar y analizar datos para proporcionar respuestas precisas a las preguntas que se me hacen. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Cuál es mi nombre?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - `ConversationTokenBufferMemory`\n",
    "\n",
    "Mantiene un búfer de interacciones recientes en la memoria y utiliza la longitud de tokens en lugar del nº de interacciones para determinar cuándo \"cortar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "buffer_token_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40) #Guarda los últimos X tokens\n",
    "buffer_token_memory.save_context({\"input\": \"¿Cómo se llama el planeta en el que vivimos?\"},\n",
    "                                 {\"output\": \"La Tierra\"})\n",
    "buffer_token_memory.save_context({\"input\": \"¿En qué galaxia se sitúa?\"},\n",
    "                                 {\"output\": \"La Vía Láctea\"})\n",
    "buffer_token_memory.save_context({\"input\": \"¿En qué sistema?\"},\n",
    "                                 {\"output\": \"En el Sistema Solar\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como cambia el contexto según cambiamos el nº máx de tokens permitidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: La Tierra\\nHuman: ¿En qué galaxia se sitúa?\\nAI: La Vía Láctea\\nHuman: ¿En qué sistema?\\nAI: En el Sistema Solar'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_token_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - `ConversationSummaryMemory`\n",
    "\n",
    "En lugar de limitar el nº de intercambios o tokens, podemos escribir un resumen de la conversación hasta ahora y lo usaremos como memoria. \n",
    "\n",
    "Este enfoque mantiene la última interacción en la memoria y resume las anteriores. Así que, cuando llega a una nueva interacción, la anterior se combina con el resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human and AI engage in small talk, with the AI asking how the human is doing and the human responding that they are just passing time. The AI then informs the human of their schedule for the day, including a meeting with the engineering team, working on a Python project, and having lunch with a client to showcase a demo of the LLM model.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "agenda = \"\"\"\n",
    "- Hay una reunión a las 10 a.m. con el equipo de ingeniería, así que necesitas tener preparada la presentación PowerPoint.\n",
    "- De 11 a.m. a 13 p.m. hay que trabajar en el proyecto de Python, lo cuál será rápido porque el proyecto está casi terminado.\n",
    "- Al mediodía, almuerzo en el restaurante italiano con un cliente que ha conducido más de una hora para poder reunirse contigo. Asegúrate de llevar el portátil para mostrar la última demo del modelo LLM que has preparado.\n",
    "\"\"\"\n",
    "buffer_summary_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "buffer_summary_memory.save_context({\"input\": \"Hola\"}, {\"output\": \"¿Qué tal?\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"No mucho, solo pasando el rato\"},\n",
    "                                   {\"output\": \"Genial\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"¿Qué haces en la agenda?\"}, \n",
    "                                   {\"output\": f\"{agenda}\"})\n",
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain tiene varios prompts por defecto escritos en inglés que pueden modificar la memoria o la salida del modelo. Por ello, en casos como este, podemos actualizar el prompt para que se adecue más a nuestras necesidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['new_lines', 'summary'], template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory.prompt import SUMMARY_PROMPT\n",
    "\n",
    "SUMMARY_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nEl humano y la IA conversan sobre la agenda del día. La IA menciona una reunión con el equipo de ingeniería a las 10 a.m. y trabajar en un proyecto de Python de 11 a.m. a 1 p.m. También hay un almuerzo con un cliente en un restaurante italiano al mediodía para mostrar una demo del modelo LLM.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_template = \"\"\"\n",
    "Resume progresivamente las líneas de conversación proporcionadas, añadiendo al resumen anterior para devolver un nuevo resumen.\n",
    "\n",
    "EJEMPLO\n",
    "Resumen actual:\n",
    "El humano pregunta qué piensa la IA sobre la Inteligencia Artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien.\n",
    "\n",
    "Nuevas líneas de conversación:\n",
    "Humano: ¿Por qué piensas que la inteligencia artificial es una fuerza para el bien?\n",
    "IA: Porque la inteligencia artificial ayudará a los humanos a alcanzar su máximo potencial.\n",
    "\n",
    "Nuevo resumen:\n",
    "El humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien porque ayudará a alcanzar su máximo potencial.\n",
    "FIN DEL EJEMPLO\n",
    "\n",
    "Resumen actual:\n",
    "{summary}\n",
    "\n",
    "Nuevas líneas de conversación:\n",
    "{new_lines}\n",
    "\n",
    "Nuevo resumen:\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "buffer_summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100,\n",
    "    prompt=summary_prompt,\n",
    ")\n",
    "buffer_summary_memory.save_context({\"input\": \"Hola\"}, {\"output\": \"¿Qué tal?\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"No mucho, solo pasando el rato\"}, {\"output\": \"Genial\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"¿Qué hay en la agenda de hoy?\"}, {\"output\": f\"{agenda}\"})\n",
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "System: \n",
      "El humano y la IA conversan sobre la agenda del día. La IA menciona una reunión con el equipo de ingeniería a las 10 a.m. y trabajar en un proyecto de Python de 11 a.m. a 1 p.m. También hay un almuerzo con un cliente en un restaurante italiano al mediodía para mostrar una demo del modelo LLM.\n",
      "Humano: ¿Tengo una reunión a las 10 am?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Sí, tienes una reunión con el equipo de ingeniería a las 10 a.m. Es una reunión de seguimiento para discutir el progreso del proyecto de inteligencia artificial que estamos desarrollando. También se discutirán posibles desafíos y soluciones. ¿Tienes alguna pregunta sobre la reunión?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=buffer_summary_memory,\n",
    "    verbose=True,\n",
    "    prompt= chat_prompt,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"¿Tengo una reunión a las 10 am?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Conversación actual:\n",
      "System: \n",
      "El humano y la IA conversan sobre la agenda del día. La IA menciona una reunión con el equipo de ingeniería a las 10 a.m. y trabajar en un proyecto de Python de 11 a.m. a 1 p.m. También hay un almuerzo con un cliente en un restaurante italiano al mediodía para mostrar una demo del modelo LLM.\n",
      "Human: ¿Tengo una reunión a las 10 am?\n",
      "AI: : Sí, tienes una reunión con el equipo de ingeniería a las 10 a.m. Es una reunión de seguimiento para discutir el progreso del proyecto de inteligencia artificial que estamos desarrollando. También se discutirán posibles desafíos y soluciones. ¿Tienes alguna pregunta sobre la reunión?\n",
      "Humano: ¿Cuántas diapositivas aconsejas para mi presentación?\n",
      "IA\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': Según mi análisis de las presentaciones anteriores, recomendaría entre 10 y 15 diapositivas. Esto permitirá una presentación concisa y enfocada en los aspectos más importantes del proyecto. Sin embargo, siempre puedes agregar o eliminar diapositivas según sea necesario. ¿Hay algo en particular que quieras enfocar en tu presentación?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuántas diapositivas aconsejas para mi presentación?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: El humano y la IA conversan sobre la agenda del día. La IA menciona una reunión con el equipo de ingeniería a las 10 a.m. y trabajar en un proyecto de Python de 11 a.m. a 1 p.m. También hay un almuerzo con un cliente en un restaurante italiano al mediodía para mostrar una demo del modelo LLM. El humano pregunta sobre la reunión a las 10 a.m. y la IA explica que es una reunión de seguimiento para discutir el progreso del proyecto de inteligencia artificial. También menciona que se discutirán posibles desafíos y soluciones. El humano luego pregunta sobre la cantidad de diapositivas recomendadas para su presentación y la IA responde que depende de la información que quiera incluir y que puede ayudar a revisar y editar la presentación si es necesario.\\nAI: : Según mi análisis de las presentaciones anteriores, recomendaría entre 10 y 15 diapositivas. Esto permitirá una presentación concisa y enfocada en los aspectos más importantes del proyecto. Sin embargo, siempre puedes agregar o eliminar diapositivas según sea necesario. ¿Hay algo en particular que quieras enfocar en tu presentación?'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Memoria conversacional con entidades\n",
    "\n",
    "## 2.1 - `ConversationEntityMemory`\n",
    "\n",
    "Esta memoria extrae info sobre las entidades (utilizando LLMs) y construye su conocimiento sobre cada entidad según avanza la conversación.\n",
    "\n",
    "Hemos preparado un ej. con 5 entidades (conocidas de antemano por el modelo y desconocidas)\n",
    "\n",
    "    * Jorge (desconocido)\n",
    "    * Ignacio (desconocido)\n",
    "    * Langchain (desconocido en 2021, es edecir, datos del último modelo)\n",
    "    * Nielsen (conocido)\n",
    "    * Everest (conocido)\n",
    "\n",
    "Al generar el resumen de las entidades, podemos ver cómo el modelo incorpora info adicional a las entidades \"conocidas\" dada la info contextual proporcionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "# Mostramos el template de base (en inglés)\n",
    "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro primer paso, como en casos anteriores, es modificar adecuadamente el prompt para que se encuentre en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_prompt_template = \"\"\"\n",
    "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
    "\n",
    "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones en un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
    "\n",
    "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
    "\n",
    "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
    "\n",
    "Contexto:\n",
    "{entities}\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Última línea:\n",
    "Humano: {input}\n",
    "Tú:\n",
    "\"\"\"\n",
    "\n",
    "entity_prompt = PromptTemplate.from_template(entity_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "entity_memory = ConversationEntityMemory(llm=llm)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    prompt = entity_prompt,\n",
    "    memory = entity_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones en un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "{'Jorge': '', 'Ignacio': '', 'Nielsen': ''}\n",
      "\n",
      "Conversación actual:\n",
      "\n",
      "Última línea:\n",
      "Humano: Jorge e Ignacio están trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEso suena como un equipo muy diverso y talentoso. ¿Puedo ayudar con algo relacionado con su proyecto?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Jorge e Ignacio están trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jorge': 'Jorge es economista y está trabajando en un proyecto para Nielsen.',\n",
       " 'Ignacio': 'Ignacio es un ingeniero que está trabajando en un proyecto para Nielsen junto a Jorge, un economista.',\n",
       " 'Nielsen': 'Nielsen es una empresa para la que Jorge e Ignacio están trabajando en un proyecto, con Jorge como economista e Ignacio como ingeniero.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones en un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "{'Nielsen': 'Nielsen es una empresa para la que Jorge e Ignacio están trabajando en un proyecto, con Jorge como economista e Ignacio como ingeniero.'}\n",
      "\n",
      "Conversación actual:\n",
      "Human: Jorge e Ignacio están trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\n",
      "AI: \n",
      "Eso suena como un equipo muy diverso y talentoso. ¿Puedo ayudar con algo relacionado con su proyecto?\n",
      "Última línea:\n",
      "Humano: ¿Qué tipo de empresa es Nielsen?\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNielsen es una empresa global de medición y análisis de datos que proporciona información y conocimientos sobre el comportamiento del consumidor y el rendimiento de los medios.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Qué tipo de empresa es Nielsen?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones en un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "{'Jorge': 'Jorge es economista y está trabajando en un proyecto para Nielsen.', 'Everest': ''}\n",
      "\n",
      "Conversación actual:\n",
      "Human: Jorge e Ignacio están trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\n",
      "AI: \n",
      "Eso suena como un equipo muy diverso y talentoso. ¿Puedo ayudar con algo relacionado con su proyecto?\n",
      "Human: ¿Qué tipo de empresa es Nielsen?\n",
      "AI: \n",
      "Nielsen es una empresa global de medición y análisis de datos que proporciona información y conocimientos sobre el comportamiento del consumidor y el rendimiento de los medios.\n",
      "Última línea:\n",
      "Humano: Jorge tiene el sueño de escalar el monte Everest\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEso es impresionante. ¿Sabías que el monte Everest es la montaña más alta del mundo con una altura de 8,848 metros? ¿Qué te motiva a Jorge a querer escalarla?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Jorge tiene el sueño de escalar el monte Everest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones en un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "{'Kyllian Mbappe': ''}\n",
      "\n",
      "Conversación actual:\n",
      "Human: Jorge e Ignacio están trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\n",
      "AI: \n",
      "Eso suena como un equipo muy diverso y talentoso. ¿Puedo ayudar con algo relacionado con su proyecto?\n",
      "Human: ¿Qué tipo de empresa es Nielsen?\n",
      "AI: \n",
      "Nielsen es una empresa global de medición y análisis de datos que proporciona información y conocimientos sobre el comportamiento del consumidor y el rendimiento de los medios.\n",
      "Human: Jorge tiene el sueño de escalar el monte Everest\n",
      "AI: \n",
      "Eso es impresionante. ¿Sabías que el monte Everest es la montaña más alta del mundo con una altura de 8,848 metros? ¿Qué te motiva a Jorge a querer escalarla?\n",
      "Última línea:\n",
      "Humano: ¿Quién es Kyllian Mbappe?\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kyllian Mbappe es un futbolista francés que juega como delantero para el Paris Saint-Germain y la selección nacional de Francia. Es considerado uno de los mejores jugadores jóvenes del mundo y ha ganado numerosos premios y títulos a lo largo de su carrera.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Quién es Kyllian Mbappe?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jorge': 'Jorge tiene el sueño de escalar el monte Everest.',\n",
       " 'Ignacio': 'Ignacio es un ingeniero que está trabajando en un proyecto para Nielsen junto a Jorge, un economista.',\n",
       " 'Nielsen': 'Nielsen es una empresa global de medición y análisis de datos que proporciona información y conocimientos sobre el comportamiento del consumidor y el rendimiento de los medios.',\n",
       " 'Everest': 'El monte Everest es la montaña más alta del mundo con una altura de 8,848 metros y Jorge tiene el sueño de escalarla.',\n",
       " 'Kyllian Mbappe': 'Kyllian Mbappe es un futbolista francés que juega como delantero para el Paris Saint-Germain y la selección nacional de Francia. Es considerado uno de los mejores jugadores jóvenes del mundo y ha ganado numerosos premios y títulos a lo largo de su carrera.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Memoria basada con un grafo de conocimiento\n",
    "## 3.1 - `ConversationKGMemory` \n",
    "En este caso utilizamos el modelo va generando un pseudo grafo de conocimiento (en formato de texto) donde va almacenando la informacion relevante de la conversación, centrandose sobretodo en las diferentes entidades.\n",
    "\n",
    "Vamos a utilizar el template que hicimos anteriormente pero sin la variable entities, ya que no es utilizada por `ConversationKGMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_prompt_template = \"\"\"\n",
    "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
    "\n",
    "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
    "\n",
    "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
    "\n",
    "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
    "\n",
    "Contexto:\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Última línea:\n",
    "Humano: {input}\n",
    "Tú:\n",
    "\"\"\"\n",
    "\n",
    "kg_prompt = PromptTemplate.from_template(kg_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "kg_memory = ConversationKGMemory(llm=llm)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    prompt=kg_prompt,\n",
    "    memory=kg_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "\n",
      "Conversación actual:\n",
      "\n",
      "Última línea:\n",
      "Humano: Jorge e Ignacio estan trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "\n",
      "Conversación actual:\n",
      "\n",
      "Última línea:\n",
      "Humano: Que tipo de empresa es Nielsen?\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "\n",
      "Conversación actual:\n",
      "On Jorge: Jorge está trabajando en un proyecto para Nielsen. Jorge es economista.\n",
      "Última línea:\n",
      "Humano: Jorge tiene el sueño de escalar el monte Everest\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEso es impresionante, el monte Everest es una de las montañas más altas del mundo. ¿Sabías que solo alrededor del 4% de las personas que intentan escalarlo lo logran? Es un gran desafío, pero estoy seguro de que Jorge tiene las habilidades y la determinación para lograrlo. Además, como economista, probablemente tenga una perspectiva única sobre el impacto económico del turismo en la región del Everest. ¿Qué te gustaría saber más sobre su proyecto para Nielsen?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Jorge e Ignacio estan trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\")\n",
    "conversation.predict(input=\"Que tipo de empresa es Nielsen?\")\n",
    "conversation.predict(input=\"Jorge tiene el sueño de escalar el monte Everest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
      "\n",
      "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
      "\n",
      "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
      "\n",
      "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
      "\n",
      "Contexto:\n",
      "\n",
      "Conversación actual:\n",
      "On Ignacio: Ignacio está trabajando en un proyecto para Nielsen. Ignacio es ingeniero.\n",
      "Última línea:\n",
      "Humano: ¿Qué sabes de Ignacio?\n",
      "Tú:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIgnacio es un ingeniero que está trabajando en un proyecto para Nielsen.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='¿Qué sabes de Ignacio?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jorge', 'un proyecto para Nielsen', 'está trabajando en'),\n",
       " ('Jorge', 'economista', 'es'),\n",
       " ('Jorge', 'escalar el monte Everest', 'tiene el sueño de'),\n",
       " ('Ignacio', 'un proyecto para Nielsen', 'está trabajando en'),\n",
       " ('Ignacio', 'ingeniero', 'es'),\n",
       " ('Ignacio', 'engineer', 'is an'),\n",
       " ('Ignacio', 'a project', 'is working on'),\n",
       " ('Ignacio', 'Nielsen', 'is working for'),\n",
       " ('Nielsen', 'empresa', 'es una'),\n",
       " ('Nielsen', 'medición y análisis de datos', 'es una empresa de'),\n",
       " ('Nielsen', 'información y conocimientos', 'proporciona'),\n",
       " ('Nielsen', 'una amplia gama de servicios', 'ofrece')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_memory.kg.get_triples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcionalidad es similar a la que ofrece `ConversationEntityMemory`, pero organiza la informacion en forma de tripletas de un grafo de conocimiento.\n",
    "\n",
    "Además de ofrecernos la posibilidad de ver el grafo de conocimiento que el LLM va generando, LangChain nos ofrece metodos para interactuar con el mismo:\n",
    "\n",
    "    * Extraccion de entidades\n",
    "    * Extraccion de tripletas\n",
    "\n",
    "\n",
    "Pero mi experiencia personal es que no funciona muy bien, ya que no está 100% relacionado con el grafo que tiene en memoria por lo que resulta en un comportamiento extraño. Mi consejo en este caso seria hacer algo adhoc en vez de utilizar estos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ignacio', 'Nielsen', 'Google']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_memory.get_current_entities({\"input\":\"Que sabes de Ignacio, Nielsen y Google?\"}) # Extrae entidades de una frase, pero no tiene porque estar relacionado con su KG interno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='Ignacio', predicate='is an', object_='engineer'),\n",
       " KnowledgeTriple(subject='Ignacio', predicate='works on', object_='a project'),\n",
       " KnowledgeTriple(subject='Ignacio', predicate='works for', object_='Nielsen'),\n",
       " KnowledgeTriple(subject='Nielsen', predicate='is a', object_='company'),\n",
       " KnowledgeTriple(subject='Nielsen', predicate='works with', object_='Google')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_memory.get_knowledge_triplets({\"input\":\"Que sabes de Ignacio, Nielsen y Google?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Memoria basada en bases de datos vectoriales\n",
    "## 4.1 - `VectorStoreRetrieverMemory` \n",
    "`VectorStoreRetrieverMemory` almacena recuerdos en un VectorDB y consulta los documentos más \"relevantes\" principales cada vez que se llama. Esto difiere de la mayoría de las otras clases de memoria en que no sigue explícitamente el orden de las interacciones.\n",
    "\n",
    "En este caso, los \"documentos\" son fragmentos anteriores de la conversación. Esto puede ser útil para referirse a piezas relevantes de información que la IA recibió anteriormente en la conversación.\n",
    "\n",
    "pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.\n",
    "\n",
    " OR\n",
    " \n",
    "pip install faiss-cpu # For CPU Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-large\",\n",
    "    openai_api_key = getenv('OPEN_API_KEY', '')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "text = \"Esto es una prueba\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(len(query_result)) #Devuelve un vector de dim 3072\n",
    "#query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar estos embeddings con una BBDD vectorial como FAISS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "embedding_size = 3072\n",
    "index = IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings, index, InMemoryDocstore({}), {})\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs=dict(k=2)) #Devolvemos K documentos\n",
    "vector_memory = VectorStoreRetrieverMemory(retriever=vector_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_memory.save_context({\"Humano\": \"Mi comida favorita el es pollo asado\"}, \n",
    "                           {\"IA\": \"Magnífica elección\"})\n",
    "vector_memory.save_context({\"Humano\": \"Todas las tardes juego al fútbol\"}, \n",
    "                           {\"IA\": \"El fútbol es muy entretenido\"})\n",
    "vector_memory.save_context({\"Humano\": \"No me gusta el golf\"}, \n",
    "                           {\"IA\": \"...\"})\n",
    "vector_memory.save_context({\"Humano\": \"Mi ciudad favorita es Zagreb\"}, \n",
    "                           {\"IA\": \"Estupendo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humano: Todas las tardes juego al fútbol\n",
      "IA: El fútbol es muy entretenido\n",
      "Humano: No me gusta el golf\n",
      "IA: ...\n"
     ]
    }
   ],
   "source": [
    "print(vector_memory.load_memory_variables({\"prompt\": \"Que deporte me aconsejas ver en la tele\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos como usarlo con una cadena conversacional. Para ello, vamos a modificar levemente el prompt de tal forma que el modelo utilice los \"documentos\" recuperados para basar su respuesta si lo considera adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_prompt_template = \"\"\"\n",
    "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. \\\n",
    "La IA es conversadora y proporciona muchos detalles específicos de su contexto. \\\n",
    "Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
    "\n",
    "Interacciones previas en la conversación que son relevantes:\n",
    "{history}\n",
    "\n",
    "(No necesitas utilizar estas interacciones previas para basar tu respuesta si no lo consideras relevante)\n",
    "\n",
    "Conversación actual:\n",
    "Humano: {input}\n",
    "IA:\n",
    "\"\"\"\n",
    "\n",
    "vector_prompt = PromptTemplate.from_template(vector_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    prompt = vector_prompt,\n",
    "    memory = vector_memory,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
      "\n",
      "Interacciones previas en la conversación que son relevantes:\n",
      "Humano: Todas las tardes juego al fútbol\n",
      "IA: El fútbol es muy entretenido\n",
      "Humano: No me gusta el golf\n",
      "IA: ...\n",
      "\n",
      "(No necesitas utilizar estas interacciones previas para basar tu respuesta si no lo consideras relevante)\n",
      "\n",
      "Conversación actual:\n",
      "Humano: ¿Qué deporte me gusta?\n",
      "IA:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSegún nuestras interacciones previas, sé que no te gusta el golf. Pero basándome en tus actividades diarias, puedo inferir que te gusta el fútbol. ¿Es correcto?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Qué deporte me gusta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain-NIv1GH6l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
